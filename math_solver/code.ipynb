{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "google_api_key = 'AIzaSyB-1LHMRO0MkrOCn9m0TUxCQITVfsyDJUA'\n",
    "langsmith = \"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\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import getpass\n",
    "\n",
    "os.environ[\"LANGSMITH_TRACING\"] = \"true\"\n",
    "os.environ[\"LANGSMITH_API_KEY\"] = langsmith\n",
    "os.environ[\"LANGSMITH_ENDPOINT\"]=\"https://api.smith.langchain.com\"\n",
    "os.environ[\"LANGSMITH_PROJECT\"]=\"mathbot\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _set_env(var: str):\n",
    "    if not os.environ.get(var):\n",
    "        os.environ[var] = getpass.getpass(f\"{var}: \")\n",
    "\n",
    "\n",
    "_set_env(\"google_api_key\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AIMessage(content=\"J'adore la programmation.\", additional_kwargs={}, response_metadata={'prompt_feedback': {'block_reason': 0, 'safety_ratings': []}, 'finish_reason': 'STOP', 'safety_ratings': []}, id='run-311a317a-3245-4936-b623-b00a66b62317-0', usage_metadata={'input_tokens': 20, 'output_tokens': 7, 'total_tokens': 27, 'input_token_details': {'cache_read': 0}})"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain_google_genai import ChatGoogleGenerativeAI\n",
    "\n",
    "llm = ChatGoogleGenerativeAI(model=\"gemini-2.0-flash\")\n",
    "messages = [\n",
    "    (\n",
    "        \"system\",\n",
    "        \"You are a helpful assistant that translates English to French. Translate the user sentence.\",\n",
    "    ),\n",
    "    (\"human\", \"I love programming.\"),\n",
    "]\n",
    "ai_msg = llm.invoke(messages)\n",
    "ai_msg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import TypedDict, List, Optional\n",
    "\n",
    "class AgentState(TypedDict):\n",
    "    problem: str\n",
    "    complexity: str\n",
    "    sub_problems: List[str]\n",
    "    current_problem: str\n",
    "    task: str\n",
    "    generated_code: List[str]\n",
    "    safe_mode: bool\n",
    "    solutions: List[str]\n",
    "    context: dict\n",
    "    current_output: Optional[str]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "llm = ChatGoogleGenerativeAI(model=\"gemini-2.0-flash\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "\n",
    "def classify_problem(state: AgentState):\n",
    "    prompt = ChatPromptTemplate.from_template(\"\"\"\n",
    "        Your task is to classify the math problem as 'simple' or 'complex'.\n",
    "        \n",
    "        - Respond **only** with 'simple' if the problem does not require multiple steps and involves easy calculations.\n",
    "        - Respond **only** with 'complex' if the problem requires breaking down into multiple steps.\n",
    "        - Respond **only** with 'NOT A MATH PROBLEM, DON'T FOOL WITH ME' if the input is not a math problem.\n",
    "        \n",
    "        Do **not** include any explanations or additional text in your response.\n",
    "        \n",
    "        Problem: {problem}\n",
    "    \"\"\")\n",
    "\n",
    "    response = llm.invoke(prompt.format(problem=state[\"problem\"]))\n",
    "    classification = response.content.strip().lower()\n",
    "\n",
    "    return {\"complexity\": classification}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "def decompose_problem(state: AgentState):\n",
    "    prompt = ChatPromptTemplate.from_template(\"\"\"\n",
    "    Break a complex problem into sub-problems.\n",
    "    Respond with bullet points only.\n",
    "    These subproblems should contain a mathematical operation or other theory at a time.\n",
    "    Do **not** include any explanations or additional text in your response.\n",
    "    Don't make unneccesary very easy sub-problems. I am dividing the problem into sub-problem so that it makes\n",
    "    it easy to solve by llm.\n",
    "    If a sub-problem requires data from previous sub-problem, report to use it, in that sub-problem only.\n",
    "    Don't make new sub-problem for it.\n",
    "    The problem is {problem}                                                                                                                               \n",
    "    \"\"\")\n",
    "\n",
    "    response = llm.invoke(prompt.format(problem=state[\"problem\"]))\n",
    "    \n",
    "    sub_problems = [line.strip(\"- \") for line in response.content.split(\"\\n\")]\n",
    "    total_count = len(sub_problems)\n",
    "    return {\"sub_problems\": sub_problems, \"total_count\": total_count}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "def substitute_context(problem: str, context: dict):\n",
    "    # Replace placeholders with actual values\n",
    "    for i, (key, value) in enumerate(context.items()):\n",
    "        problem = problem.replace(f\"[[result_{i+1}]]\", str(value))\n",
    "    \n",
    "    # Fallback to LLM-based substitution\n",
    "    if \"previous result\" in problem.lower():\n",
    "        prompt = f\"\"\"Update this problem with actual values from context:\n",
    "        Problem: {problem}\n",
    "        Context: {context}\n",
    "        Return only the updated problem\"\"\"\n",
    "        \n",
    "        response = llm.invoke(prompt)\n",
    "        return response.content.strip()\n",
    "    \n",
    "    return problem"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "def identify_problem(state: AgentState):\n",
    "    # Get next problem with context substitution\n",
    "    current_problem = substitute_context(\n",
    "        state[\"sub_problems\"].pop(0),\n",
    "        state[\"context\"]\n",
    "    )\n",
    "    \n",
    "    prompt = ChatPromptTemplate.from_template(\"\"\"\n",
    "    Analyze the problem: {problem}\n",
    "    \n",
    "    Classify as:\n",
    "    - Respond **only** with 'difficult_operation' if it requires difficult mathematical calculations that can't be easily solved by llm\n",
    "    - Respond **only** with 'easy' if it requires conceptual explanation or easy calculations\n",
    "    Do **not** include any explanations or additional text in your response.                                          \n",
    "    \"\"\")\n",
    "    \n",
    "    response = llm.invoke(prompt.format(\n",
    "        problem=current_problem,\n",
    "    ))\n",
    "\n",
    "    return {\n",
    "        \"current_problem\": current_problem,\n",
    "        \"sub_problems\": state[\"sub_problems\"],\n",
    "        \"task\": response.content.strip().lower(),\n",
    "        \"context\": state.get(\"context\", {})\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import ast\n",
    "import subprocess\n",
    "from io import StringIO\n",
    "from contextlib import redirect_stdout\n",
    "\n",
    "def code_generator(state: AgentState):\n",
    "    prompt = f\"\"\"Generate Python code for problem using data from the previous results:\n",
    "    Previous Results: {state[\"context\"]}\n",
    "    \n",
    "    Problem: {state['current_problem']}\n",
    "    Rules:\n",
    "    1. Use variables from context: {list(state[\"context\"].keys())}\n",
    "    2. Store final result in 'result' variable\n",
    "    3. Include error handling\n",
    "    \"\"\"\n",
    "    \n",
    "    response = llm.invoke(prompt)\n",
    "\n",
    "    code_match = re.search(r\"```python\\n(.*?)\\n```\", response.content, re.DOTALL)\n",
    "    return {\n",
    "        \"generated_code\": [code_match.group(1)] if code_match else [],\n",
    "        \"current_problem\": state[\"current_problem\"]\n",
    "    }\n",
    "\n",
    "def safe_executor(state: AgentState):\n",
    "    if not state[\"generated_code\"]:\n",
    "        return {\"solutions\": [\"Could not generate valid code\"]}\n",
    "    \n",
    "    code = state[\"generated_code\"][-1]\n",
    "\n",
    "    if any(banned in code for banned in [\"import os\", \"import sys\", \"__\", \"eval\"]):\n",
    "        return {\"solutions\": [\"Unsafe code detected\"]}\n",
    "\n",
    "    try:\n",
    "        tree = ast.parse(code)\n",
    "        for node in ast.walk(tree):\n",
    "            if isinstance(node, (ast.Import, ast.ImportFrom)):\n",
    "                module = node.names[0].name\n",
    "                if module not in [\"math\", \"sympy\", \"numpy\", \"scipy\"]:\n",
    "                    return {\"solutions\": [\"Invalid import detected\"]}\n",
    "\n",
    "        allowed_modules = {\n",
    "            'math': __import__('math'),\n",
    "            'sympy': __import__('sympy'),\n",
    "            'numpy': __import__('numpy'),\n",
    "            'scipy': __import__('scipy')\n",
    "        }\n",
    "\n",
    "        safe_context = {\n",
    "            str(k): v for k, v in state.get(\"context\", {}).items()\n",
    "            if isinstance(k, (str, int, float))\n",
    "        }\n",
    "        \n",
    "        exec_globals = {\n",
    "            \"__builtins__\": {\n",
    "                'None': None,\n",
    "                'True': True,\n",
    "                'False': False,\n",
    "                'abs': abs,\n",
    "                'min': min,\n",
    "                'max': max,\n",
    "                'sum': sum\n",
    "            },\n",
    "            **allowed_modules,\n",
    "            **safe_context\n",
    "        }\n",
    "\n",
    "        local_vars = {}\n",
    "        f = StringIO()\n",
    "        \n",
    "        with redirect_stdout(f):\n",
    "            exec(code, exec_globals, local_vars)\n",
    "            \n",
    "        output = f.getvalue()\n",
    "        result = local_vars.get('result', 'No result variable found')\n",
    "        \n",
    "        return {\"solutions\": [f\"Output:\\n{output}\\nResult: {result}\"]}\n",
    "    \n",
    "    except Exception as e:\n",
    "        return {\"solutions\": [f\"Execution error: {str(e)}\"]}\n",
    "\n",
    "def fallback_solver(state: AgentState):\n",
    "    if not state[\"solutions\"] or \"error\" in state[\"solutions\"][-1].lower():\n",
    "        response = llm.invoke(f\"Solve this math problem: {state['current_problem']} Please calculate carefully\")\n",
    "        return {\"solutions\": [response.content]}\n",
    "    return state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "def llm_solver(state: AgentState):\n",
    "    problem = state.get(\"current_problem\") or state[\"problem\"]\n",
    "    context = state[\"context\"]\n",
    "    \n",
    "    prompt = ChatPromptTemplate.from_template(\"\"\"\n",
    "    You are a math expert, you have to solve math problem and only give concise to the point answer.\n",
    "    Don't write unncessary things only give the solution.\n",
    "    Do **not** include any explanations or additional text in your response.\n",
    "    problem: {problem}\n",
    "    Use the data from {context} if it is needed for solving.                                                                                    \n",
    "    \"\"\")\n",
    "\n",
    "    response = llm.invoke(prompt.format(problem=problem,context=context))\n",
    "    return {\"solutions\": state[\"solutions\"] + [response.content]}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "def complexity(state: AgentState):\n",
    "    if state['complexity'] == 'complex':\n",
    "        return 'decompose'\n",
    "    elif state['complexity'] == 'simple':\n",
    "        return 'llm_solve'\n",
    "    else:\n",
    "        return \"_end_\"\n",
    "    \n",
    "def task(state: AgentState):\n",
    "    if state['task'] == 'difficult_operation':\n",
    "        return 'generate_code'\n",
    "    else:\n",
    "        return 'llm_solve'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langgraph.graph import StateGraph\n",
    "\n",
    "def build_workflow():\n",
    "    workflow = StateGraph(AgentState)\n",
    "    \n",
    "    workflow.add_node(\"classify\", classify_problem)\n",
    "    workflow.add_node(\"decompose\", decompose_problem)\n",
    "    workflow.add_node(\"identify\", identify_problem)\n",
    "    workflow.add_node(\"generate_code\", code_generator)\n",
    "    workflow.add_node(\"execute\", safe_executor)\n",
    "    workflow.add_node(\"fallback\", fallback_solver)\n",
    "    workflow.add_node(\"llm_solve\", llm_solver)\n",
    "\n",
    "    workflow.set_entry_point(\"classify\")\n",
    "    \n",
    "    # Main workflow edges\n",
    "    workflow.add_conditional_edges(\"classify\", lambda x: complexity(x))\n",
    "    workflow.add_edge(\"decompose\", \"identify\")\n",
    "    \n",
    "    # Sub-problem processing branch\n",
    "    workflow.add_conditional_edges(\"identify\", lambda x: task(x))\n",
    "    workflow.add_edge(\"generate_code\", \"execute\")\n",
    "    \n",
    "    workflow.add_conditional_edges(\n",
    "        \"execute\",\n",
    "        lambda x: \"fallback\" if is_error(x) else check_subproblems(x)\n",
    "    )\n",
    "    # Solution propagation logic\n",
    "    workflow.add_conditional_edges(\n",
    "        \"llm_solve\",\n",
    "        lambda x: check_subproblems(x)\n",
    "    )\n",
    "    workflow.add_conditional_edges(\n",
    "        \"fallback\",\n",
    "        lambda x: check_subproblems(x)\n",
    "    )\n",
    "    \n",
    "    return workflow.compile()\n",
    "\n",
    "def is_error(state: AgentState):\n",
    "    if state.get(\"solutions\"):\n",
    "        last_solution = state[\"solutions\"][-1].lower()\n",
    "        return any(err in last_solution for err in [\"error\", \"invalid\", \"unsafe\"])\n",
    "    return False\n",
    "\n",
    "def check_subproblems(state: AgentState):\n",
    "    if state[\"solutions\"]:\n",
    "        state[\"context\"][f\"step_{len(state['solutions'])}\"] = state[\"solutions\"][-1]\n",
    "    \n",
    "    # Check remaining sub-problems\n",
    "    if state[\"sub_problems\"]:\n",
    "        return \"identify\"\n",
    "    return \"__end__\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "app = build_workflow()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import display, Image\n",
    "display(Image(app.get_graph(xray=1).draw_mermaid_png()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Retrying langchain_google_genai.chat_models._chat_with_retry.<locals>._chat_with_retry in 2.0 seconds as it raised ResourceExhausted: 429 You exceeded your current quota, please check your plan and billing details. For more information on this error, head to: https://ai.google.dev/gemini-api/docs/rate-limits. [violations {\n",
      "}\n",
      ", links {\n",
      "  description: \"Learn more about Gemini API quotas\"\n",
      "  url: \"https://ai.google.dev/gemini-api/docs/rate-limits\"\n",
      "}\n",
      ", retry_delay {\n",
      "  seconds: 23\n",
      "}\n",
      "].\n"
     ]
    },
    
   ],
   "source": [
    "app.invoke(AgentState(\n",
    "    problem=\"Solve the following IVP and find the interval of validity for the solution:\\n\\\n",
    "y' = (3x^2 + 4x - 4) / (2y - 4),\\n\\\n",
    "with initial condition y(1) = 3.\",\n",
    "    complexity=\"\", \n",
    "    sub_problems=[],  \n",
    "    current_problem=\"\",\n",
    "    task=\"\",\n",
    "    generated_code=[],\n",
    "    safe_mode=True,\n",
    "    solutions=[],\n",
    "    context={},\n",
    "    current_output=None\n",
    "))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "genai",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
